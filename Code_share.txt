config_loader.py

import yaml

class ConfigLoader:
    @staticmethod
    def load_config(config_path):
        with open(config_path, 'r') as file:
            return yaml.safe_load(file)


intelligence_program.py

import boto3

class IntelligenceProgram:
    @staticmethod
    def get_s3_folder_size(s3_path):
        s3 = boto3.client('s3')
        bucket_name = s3_path.split('/')[2]
        prefix = '/'.join(s3_path.split('/')[3:])
        total_size = 0

        paginator = s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
            for obj in page.get('Contents', []):
                total_size += obj['Size']

        return total_size / (1024 * 1024)  # Convert bytes to MB

    @classmethod
    def generate_operator_details(cls, task, default_input_paths, default_output_path):
        if 'size' not in task:
            input_paths = task.get('input_data_paths', default_input_paths)
            output_path = task.get('output_data_path', default_output_path)

            input_size = sum(cls.get_s3_folder_size(path) for path in input_paths)
            output_size = cls.get_s3_folder_size(output_path)
            task['size'] = input_size + output_size

        if task['language'] == 'python':
            if task.get('use_ecs', False):
                return {
                    'id': task['id'],
                    'type': 'ECSOperator',
                    'cluster': 'your-cluster-name',
                    'task_definition': 'your-task-definition',
                    'container_name': 'your-container-name',
                    'aws_conn_id': 'your-aws-connection-id',
                    'region': 'your-region',
                    'overrides': {
                        'containerOverrides': [{
                            'name': 'your-container-name',
                            'command': ['python', task['python_callable']]
                        }]
                    },
                    'retries': 2,
                    'retry_delay': '10m',
                    'import_path': 'from airflow.providers.amazon.aws.operators.ecs import ECSOperator'
                }
            else:
                return {
                    'id': task['id'],
                    'type': 'PythonOperator',
                    'python_callable': 'example_python_callable',
                    'retries': 2,
                    'retry_delay': '10m',
                    'import_path': 'from airflow.operators.python import PythonOperator'
                }
        elif task['language'] == 'pyspark':
            return {
                'id': task['id'],
                'type': 'GlueJobOperator',
                'job_name': 'example_glue_job',
                'retries': 1,
                'retry_delay': '15m',
                'import_path': 'from airflow.providers.amazon.aws.operators.glue import GlueJobOperator'
            }
        else:
            raise ValueError(f"Unsupported language: {task['language']}")




s3_uploader.py


import boto3

class S3Uploader:
    @staticmethod
    def upload_file_to_s3(file_path, bucket_name, s3_path):
        s3 = boto3.client('s3')
        s3.upload_file(file_path, bucket_name, s3_path)





dag_builder.py

from datetime import datetime, timedelta
from jinja2 import Environment, FileSystemLoader
from airflow import DAG
from airflow.exceptions import AirflowException

class DAGBuilder:
    def __init__(self, templates_dir='operator_templates'):
        self.env = Environment(loader=FileSystemLoader(templates_dir))

    def load_template(self, template_name):
        return self.env.get_template(template_name)

    def create_task(self, dag, operator, required_imports):
        op_type = operator.get('type')
        op_id = operator.get('id')

        if not op_type or not op_id:
            raise AirflowException(f"Operator configuration is missing 'type' or 'id': {operator}")

        required_imports.add(operator['import_path'])
        template = self.load_template(f"{op_type}.j2")
        task_code = template.render(operator=operator, dag=dag)
        
        local_vars = {}
        exec(task_code, globals(), local_vars)
        return local_vars[op_id]




dag_factory.py


import os
from datetime import datetime, timedelta
from airflow import DAG
from dag_generator.config_loader import ConfigLoader
from dag_generator.dag_builder import DAGBuilder
from dag_generator.intelligence_program import IntelligenceProgram
from dag_generator.s3_uploader import S3Uploader

class DAGFactory:
    def __init__(self, config_loader=ConfigLoader, dag_builder=DAGBuilder, intelligence_program=IntelligenceProgram, s3_uploader=S3Uploader):
        self.config_loader = config_loader()
        self.dag_builder = dag_builder()
        self.intelligence_program = intelligence_program()
        self.s3_uploader = s3_uploader()

    @staticmethod
    def merge_configs(default_config, user_config):
        def recursive_merge(d, u):
            for k, v in u.items():
                if isinstance(v, dict):
                    d[k] = recursive_merge(d.get(k, {}), v)
                else:
                    d[k] = v
            return d
        return recursive_merge(default_config, user_config)

    @staticmethod
    def parse_default_args(default_args):
        if 'start_date' in default_args:
            default_args['start_date'] = datetime.strptime(default_args['start_date'], '%Y-%m-%d')
        if 'retry_delay' in default_args:
            default_args['retry_delay'] = timedelta(minutes=int(default_args['retry_delay'].replace('m', '')))
        return default_args

    def create_dag(self, dag_id, config_path):
        config = self.config_loader.load_config(config_path)

        dag_config = config.get('dag', {})
        tasks_config = config.get('tasks', [])
        notifications_config = config.get('notifications', {})

        default_input_paths = dag_config.get('input_data_paths', [])
        default_output_path = dag_config.get('output_data_path', None)

        default_args = self.parse_default_args(dag_config.get('default_args', {}))

        dag = DAG(
            dag_id=dag_id,
            description=dag_config.get('description', 'A simple example DAG'),
            schedule_interval=dag_config.get('schedule_interval', '@daily'),
            default_args=default_args,
            catchup=False
        )

        required_imports = set()
        operators_config = [self.intelligence_program.generate_operator_details(task, default_input_paths, default_output_path) for task in tasks_config]

        tasks = {op['id']: self.dag_builder.create_task(dag, op, required_imports) for op in operators_config}

        for task in tasks_config:
            op_id = task['id']
            dependencies = task.get('dependencies', [])
            for dep in dependencies:
                if dep in tasks:
                    tasks[dep] >> tasks[op_id]

        return dag, tasks, default_args, required_imports, notifications_config, dag_id

    @staticmethod
    def serialize_default_args(default_args):
        args_str = "{\n"
        for key, value in default_args.items():
            if isinstance(value, datetime):
                if value.tzinfo is not None:
                    value_str = f"datetime({value.year}, {value.month}, {value.day}, {value.hour}, {value.minute}, {value.second}, tzinfo=pytz.{value.tzinfo.zone})"
                else:
                    value_str = f"datetime({value.year}, {value.month}, {value.day}, {value.hour}, {value.minute}, {value.second})"
            elif isinstance(value, timedelta):
                value_str = f"timedelta(days={value.days}, seconds={value.seconds})"
            else:
                value_str = repr(value)
            args_str += f"    '{key}': {value_str},\n"
        args_str += "}"
        return args_str

    def generate_dag_file(self, dag_id, dag, tasks, default_args, required_imports, notifications_config, output_directory):
        dag_file_path = os.path.join(output_directory, f"{dag_id}.py")
        with open(dag_file_path, 'w') as f:
            f.write("from airflow import DAG\n")
            for imp in required_imports:
                f.write(f"{imp}\n")
            f.write("from datetime import datetime, timedelta\n")
            f.write("import pytz\n\n")
            f.write(f"default_args = {self.serialize_default_args(default_args)}\n\n")
            f.write(f"dag = DAG(dag_id='{dag_id}', default_args=default_args, description='{dag.description}', schedule_interval='{dag.schedule_interval}', catchup={dag.catchup})\n\n")
            
            for task_id, task in tasks.items():
                f.write(f"{task_id} = {task.__class__.__name__}(task_id='{task.task_id}', ")
                for key, value in task.params.items():
                    if isinstance(value, str):
                        f.write(f"{key}='{value}', ")
                    elif isinstance(value, timedelta):
                        f.write(f"{key}=timedelta(seconds={value.total_seconds()}), ")
                    else:
                        f.write(f"{key}={value}, ")
                f.write(f"dag=dag)\n")
            
            for task_id, task in tasks.items():
                for downstream_task in task.downstream_task_ids:
                    f.write(f"{task_id} >> {downstream_task}\n")

            # Add notification logic if configured
            email_notifications = notifications_config.get('email', {})
            if email_notifications:
                recipients = email_notifications.get('recipients', [])
                if recipients:
                    f.write("\ndef notify_email(context):\n")
                    f.write("    from airflow.utils.email import send_email\n")
                    f.write("    send_email(recipients, 'Airflow alert', 'Task failure alert')\n\n")
                    f.write("task_failure_email_alert = EmailOperator(\n")
                    f.write("    task_id='email_alert',\n")
                    f.write("    to=recipients,\n")
                    f.write("    subject='Airflow Task Failure',\n")
                    f.write("    html_content='Task failure alert',\n")
                    f.write("    dag=dag\n")
                    f.write(")\n")

        return dag_file_path

    def main(self, config_directory, output_directory, s3_bucket, s3_path):
        os.makedirs(output_directory, exist_ok=True)
        for config_file in os.listdir(config_directory):
            if config_file.endswith('.yaml'):
                config_path = os.path.join(config_directory, config_file)
                dag_id = os.path.splitext(config_file)[0]
                dag, tasks, default_args, required_imports, notifications_config, dag_id = self.create_dag(dag_id, config_path)
                dag_file_path = self.generate_dag_file(dag_id, dag, tasks, default_args, required_imports, notifications_config, output_directory)
                s3_dag_path = os.path.join(s3_path, f"{dag_id}.py")
                self.s3_uploader.upload_file_to_s3(dag_file_path, s3_bucket, s3_dag_path)
                print(f"DAG {dag_id} has been created and uploaded to {s3_dag_path}")

        return dag, tasks, default_args, required_imports, notifications_config, dag_id

if __name__ == "__main__":
    config_directory = '/usr/local/airflow/dags/configurations/'  # Adjust the path as needed
    output_directory = '/usr/local/airflow/dags/generated_dags/'  # Adjust the path as needed
    s3_bucket = 'your-s3-bucket'  # Replace with your S3 bucket name
    s3_path = 'dags/'  # Replace with your S3 path

    factory = DAGFactory()
    factory.main(config_directory, output_directory, s3_bucket, s3_path)



operator_templates/pythonoperator.j2

{{ operator.id }} = PythonOperator(
    task_id='{{ operator.id }}',
    python_callable={{ operator.python_callable }},
    op_args={{ operator.op_args | default([]) }},
    retries={{ operator.retries | default(1) }},
    retry_delay=timedelta(minutes={{ operator.retry_delay | default('5m') | replace('m', '') }}),
    dag=dag
)


gluejoboperator.j2

{{ operator.id }} = GlueJobOperator(
    task_id='{{ operator.id }}',
    job_name='{{ operator.job_name }}',
    retries={{ operator.retries | default(1) }},
    retry_delay=timedelta(minutes={{ operator.retry_delay | default('5m') | replace('m', '') }}),
    dag=dag
)


ecsoperator


{{ operator.id }} = ECSOperator(
    task_id='{{ operator.id }}',
    cluster='{{ operator.cluster }}',
    task_definition='{{ operator.task_definition }}',
    container_name='{{ operator.container_name }}',
    aws_conn_id='{{ operator.aws_conn_id }}',
    region='{{ operator.region }}',
    overrides={{ operator.overrides }},
    retries={{ operator.retries | default(1) }},
    retry_delay=timedelta(minutes={{ operator.retry_delay | default('5m') | replace('m', '') }}),
    dag=dag
)






