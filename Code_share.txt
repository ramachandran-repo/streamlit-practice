import os
import yaml
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException

def merge_configs(default_config, user_config):
    """
    Merge user configuration into the default configuration.
    """
    def recursive_merge(d, u):
        for k, v in u.items():
            if isinstance(v, dict):
                d[k] = recursive_merge(d.get(k, {}), v)
            else:
                d[k] = v
        return d

    return recursive_merge(default_config, user_config)

def parse_default_args(default_args):
    if 'start_date' in default_args:
        default_args['start_date'] = datetime.strptime(default_args['start_date'], '%Y-%m-%d')
    if 'retry_delay' in default_args:
        default_args['retry_delay'] = timedelta(minutes=int(default_args['retry_delay'].replace('m', '')))
    return default_args

def create_task(dag, operator):
    op_type = operator.get('type')
    op_id = operator.get('id')

    if not op_type or not op_id:
        raise AirflowException(f"Operator configuration is missing 'type' or 'id': {operator}")

    task_params = {
        'task_id': op_id,
        'retries': operator.get('retries', 1),
        'retry_delay': timedelta(minutes=int(operator.get('retry_delay', '5m').replace('m', ''))),
        'dag': dag
    }

    if op_type == 'BashOperator':
        return BashOperator(bash_command=operator['bash_command'], **task_params)
    elif op_type == 'PythonOperator':
        return PythonOperator(python_callable=eval(operator['python_callable']), op_args=operator.get('op_args', []), **task_params)
    else:
        raise AirflowException(f"Unsupported operator type: {op_type}")

def create_dag(dag_id, config_path):
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)

    default_config = config.get('dag', {})
    user_config = config.get('user_config', {}).get('dag', {})

    final_config = merge_configs(default_config, user_config)
    default_args = parse_default_args(final_config.get('default_args', {}))

    dag = DAG(
        dag_id=dag_id,
        description=final_config.get('description', 'A simple example DAG'),
        schedule_interval=final_config.get('schedule_interval', '@daily'),
        default_args=default_args,
        catchup=False
    )

    operators_config = config.get('operators', [])
    user_operators_config = config.get('user_config', {}).get('operators', [])
    operators_config.extend(user_operators_config)

    tasks = {op['id']: create_task(dag, op) for op in operators_config}

    for operator in operators_config:
        op_id = operator['id']
        dependencies = operator.get('dependencies', [])
        for dep in dependencies:
            if dep in tasks:
                tasks[dep] >> tasks[op_id]

    return dag, tasks

def generate_dag_file(dag_id, dag, tasks, output_directory):
    dag_file_path = os.path.join(output_directory, f"{dag_id}.py")
    with open(dag_file_path, 'w') as f:
        f.write("from airflow import DAG\n")
        f.write("from airflow.operators.bash import BashOperator\n")
        f.write("from airflow.operators.python import PythonOperator\n")
        f.write("from datetime import datetime, timedelta\n\n")
        f.write(f"default_args = {dag.default_args}\n\n")
        f.write(f"dag = DAG(dag_id='{dag.dag_id}', default_args=default_args, description='{dag.description}', schedule_interval='{dag.schedule_interval}', catchup={dag.catchup})\n\n")
        for task_id, task in tasks.items():
            if isinstance(task, BashOperator):
                f.write(f"{task_id} = BashOperator(task_id='{task.task_id}', bash_command=\"{task.bash_command}\", retries={task.retries}, retry_delay=timedelta(minutes={task.retry_delay.seconds // 60}), dag=dag)\n")
            elif isinstance(task, PythonOperator):
                f.write(f"{task_id} = PythonOperator(task_id='{task.task_id}', python_callable={task.python_callable.__name__}, op_args={task.op_args}, retries={task.retries}, retry_delay=timedelta(minutes={task.retry_delay.seconds // 60}), dag=dag)\n")
        for task_id, task in tasks.items():
            for downstream_task in task.downstream_task_ids:
                f.write(f"{task_id} >> {downstream_task}\n")

def generate_dags(config_directory, output_directory):
    os.makedirs(output_directory, exist_ok=True)
    for config_file in os.listdir(config_directory):
        if config_file.endswith('.yaml'):
            config_path = os.path.join(config_directory, config_file)
            dag_id = os.path.splitext(config_file)[0]
            dag, tasks = create_dag(dag_id, config_path)
            generate_dag_file(dag_id, dag, tasks, output_directory)

if __name__ == "__main__":
    config_directory = '/usr/local/airflow/dags/configurations/'  # Adjust the path as needed
    output_directory = '/usr/local/airflow/dags/generated_dags/'  # Adjust the path as needed
    generate_dags(config_directory, output_directory)
