from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.sensors.s3_key_sensor import S3KeySensor
import boto3

def trigger_new_dag(dag_id, **kwargs):
    client = boto3.client('mwaa')
    client.create_cli_token(Name='mwaa-env-name')  # replace with your MWAA environment name
    # Construct the Airflow CLI command
    command = f'airflow dags trigger {dag_id}'
    client.execute_command(Command=command)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'monitor_new_dags',
    default_args=default_args,
    description='A DAG to monitor and trigger new DAGs',
    schedule_interval=timedelta(minutes=1),
    catchup=False,
)

sensor = S3KeySensor(
    task_id='check_for_new_dag',
    bucket_key='s3://my-bucket/dags/new_dag_file.py',  # Specify the S3 path to the new DAG file
    wildcard_match=True,
    aws_conn_id='aws_default',
    timeout=18 * 60 * 60,
    poke_interval=60,
    dag=dag,
)

trigger = PythonOperator(
    task_id='trigger_new_dag',
    python_callable=trigger_new_dag,
    op_kwargs={'dag_id': 'my_new_dag'},  # Specify the new DAG ID
    dag=dag,
)

sensor >> trigger
