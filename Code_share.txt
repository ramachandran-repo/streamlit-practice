import boto3
import json
from airflow.hooks.S3_hook import S3Hook

def list_new_dag_files(bucket_name, processed_files_key, prefix='dags/'):
    s3_hook = S3Hook(aws_conn_id='aws_default')
    s3_client = s3_hook.get_conn()
    
    # List all DAG files in the bucket
    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
    all_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.py')]

    # Load the list of already processed files
    try:
        processed_files_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_files_key)
        processed_files = json.loads(processed_files_obj['Body'].read().decode('utf-8'))
    except s3_client.exceptions.NoSuchKey:
        processed_files = []

    # Identify new files
    new_files = list(set(all_files) - set(processed_files))

    # Update the list of processed files
    s3_client.put_object(Bucket=bucket_name, Key=processed_files_key, Body=json.dumps(all_files))

    return new_files
