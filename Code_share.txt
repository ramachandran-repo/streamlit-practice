{{ operator.id }} = EmailOperator(
    task_id='{{ operator.id }}',
    to='{{ operator.to }}',
    subject='{{ operator.subject }}',
    html_content='{{ operator.html_content }}',
    retries={{ operator.retries | default(1) }},
    retry_delay=timedelta(minutes={{ operator.retry_delay | default('5m') | replace('m', '') }}),
    dag=dag
)



import os
from datetime import datetime, timedelta
from airflow import DAG
from dag_generator.config_loader import ConfigLoader
from dag_generator.dag_builder import DAGBuilder
from dag_generator.intelligence_program import IntelligenceProgram
from dag_generator.s3_uploader import S3Uploader

class DAGFactory:
    def __init__(self, config_loader=ConfigLoader, dag_builder=DAGBuilder, intelligence_program=IntelligenceProgram, s3_uploader=S3Uploader):
        self.config_loader = config_loader()
        self.dag_builder = dag_builder()
        self.intelligence_program = intelligence_program()
        self.s3_uploader = s3_uploader()

    @staticmethod
    def merge_configs(default_config, user_config):
        def recursive_merge(d, u):
            for k, v in u.items():
                if isinstance(v, dict):
                    d[k] = recursive_merge(d.get(k, {}), v)
                else:
                    d[k] = v
            return d
        return recursive_merge(default_config, user_config)

    @staticmethod
    def parse_default_args(default_args):
        if 'start_date' in default_args:
            default_args['start_date'] = datetime.strptime(default_args['start_date'], '%Y-%m-%d')
        if 'retry_delay' in default_args:
            default_args['retry_delay'] = timedelta(minutes=int(default_args['retry_delay'].replace('m', '')))
        return default_args

    def create_dag(self, dag_id, config_path):
        config = self.config_loader.load_config(config_path)

        dag_config = config.get('dag', {})
        tasks_config = config.get('tasks', [])
        notifications_config = config.get('notifications', {})

        default_input_paths = dag_config.get('input_data_paths', [])
        default_output_path = dag_config.get('output_data_path', None)

        default_args = self.parse_default_args(dag_config.get('default_args', {}))

        dag = DAG(
            dag_id=dag_id,
            description=dag_config.get('description', 'A simple example DAG'),
            schedule_interval=dag_config.get('schedule_interval', '@daily'),
            default_args=default_args,
            catchup=False
        )

        required_imports = set()
        operators_config = [self.intelligence_program.generate_operator_details(task, default_input_paths, default_output_path) for task in tasks_config]

        tasks = {op['id']: self.dag_builder.create_task(dag, op, required_imports) for op in operators_config}

        for task in tasks_config:
            op_id = task['id']
            dependencies = task.get('dependencies', [])
            for dep in dependencies:
                if dep in tasks:
                    tasks[dep] >> tasks[op_id]

        # Add email notifications if configured
        email_notifications = notifications_config.get('email', {})
        if email_notifications:
            recipients = email_notifications.get('recipients', [])
            if recipients:
                email_operator_config = {
                    'id': 'task_failure_email_alert',
                    'type': 'EmailOperator',
                    'to': ','.join(recipients),
                    'subject': 'Airflow Task Failure',
                    'html_content': 'Task failure alert',
                    'retries': 1,
                    'retry_delay': '5m',
                    'import_path': 'from airflow.operators.email import EmailOperator'
                }
                email_task = self.dag_builder.create_task(dag, email_operator_config, required_imports)
                for task in tasks.values():
                    task >> email_task

        return dag, tasks, default_args, required_imports, notifications_config, dag_id

    @staticmethod
    def serialize_default_args(default_args):
        args_str = "{\n"
        for key, value in default_args.items():
            if isinstance(value, datetime):
                if value.tzinfo is not None:
                    value_str = f"datetime({value.year}, {value.month}, {value.day}, {value.hour}, {value.minute}, {value.second}, tzinfo=pytz.{value.tzinfo.zone})"
                else:
                    value_str = f"datetime({value.year}, {value.month}, {value.day}, {value.hour}, {value.minute}, {value.second})"
            elif isinstance(value, timedelta):
                value_str = f"timedelta(days={value.days}, seconds={value.seconds})"
            else:
                value_str = repr(value)
            args_str += f"    '{key}': {value_str},\n"
        args_str += "}"
        return args_str

    def generate_dag_file(self, dag_id, dag, tasks, default_args, required_imports, notifications_config, output_directory):
        dag_file_path = os.path.join(output_directory, f"{dag_id}.py")
        with open(dag_file_path, 'w') as f:
            f.write("from airflow import DAG\n")
            for imp in required_imports:
                f.write(f"{imp}\n")
            f.write("from datetime import datetime, timedelta\n")
            f.write("import pytz\n\n")
            f.write(f"default_args = {self.serialize_default_args(default_args)}\n\n")
            f.write(f"dag = DAG(dag_id='{dag_id}', default_args=default_args, description='{dag.description}', schedule_interval='{dag.schedule_interval}', catchup={dag.catchup})\n\n")
            
            for task_id, task in tasks.items():
                f.write(f"{task_id} = {task.__class__.__name__}(task_id='{task.task_id}', ")
                for key, value in task.params.items():
                    if isinstance(value, str):
                        f.write(f"{key}='{value}', ")
                    elif isinstance(value, timedelta):
                        f.write(f"{key}=timedelta(seconds={value.total_seconds()}), ")
                    else:
                        f.write(f"{key}={value}, ")
                f.write(f"dag=dag)\n")
            
            for task_id, task in tasks.items():
                for downstream_task in task.downstream_task_ids:
                    f.write(f"{task_id} >> {downstream_task}\n")

        return dag_file_path

    def main(self, config_directory, output_directory, s3_bucket, s3_path):
        os.makedirs(output_directory, exist_ok=True)
        for config_file in os.listdir(config_directory):
            if config_file.endswith('.yaml'):
                config_path = os.path.join(config_directory, config_file)
                dag_id = os.path.splitext(config_file)[0]
                dag, tasks, default_args, required_imports, notifications_config, dag_id = self.create_dag(dag_id, config_path)
                dag_file_path = self.generate_dag_file(dag_id, dag, tasks, default_args, required_imports, notifications_config, output_directory)
                s3_dag_path = os.path.join(s3_path, f"{dag_id}.py")
                self.s3_uploader.upload_file_to_s3(dag_file_path, s3_bucket, s3_dag_path)
                print(f"DAG {dag_id} has been created and uploaded to {s3_dag_path}")

        return dag, tasks, default_args, required_imports, notifications_config, dag_id

if __name__ == "__main__":
    config_directory = '/usr/local/airflow/dags/configurations/'  # Adjust the path as needed
    output_directory = '/usr/local/airflow/dags/generated_dags/'  # Adjust the path as needed
    s3_bucket = 'your-s3-bucket'  # Replace with your S3 bucket name
    s3_path = 'dags/'  # Replace with your S3 path

    factory = DAGFactory()
    factory.main(config_directory, output_directory, s3_bucket, s3_path)

