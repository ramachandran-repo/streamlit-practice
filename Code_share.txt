library(paws)
library(dplyr)
library(logger)

# Parse command-line arguments
args <- commandArgs(trailingOnly = TRUE)

if (length(args) != 2) {
  stop("Two arguments are required: input_path and output_path")
}

input_path <- args[1]
output_path <- args[2]

# Function to parse S3 URI
parse_s3_uri <- function(s3_uri) {
  matches <- regexec("s3://([^/]+)/(.*)", s3_uri)
  match <- regmatches(s3_uri, matches)
  bucket <- match[[1]][2]
  key <- match[[1]][3]
  return(list(bucket = bucket, key = key))
}

input <- parse_s3_uri(input_path)
output <- parse_s3_uri(output_path)

# Set up logging
log_appender(appender_file("log.txt"))

log_info("Setting AWS region")
Sys.setenv("AWS_DEFAULT_REGION" = "us-west-2")  # Adjust to your region

# Initialize S3 client
s3 <- paws::s3()

# Define the S3 bucket and folder for input and output
input_bucket <- input$bucket
input_folder <- input$key
output_bucket <- output$bucket
output_folder <- output$key

cat("Input bucket:", input_bucket, "\n")
cat("Input folder:", input_folder, "\n")
cat("Output bucket:", output_bucket, "\n")
cat("Output folder:", output_folder, "\n")

# List all CSV files in the input folder
cat("Listing CSV files in folder\n")
csv_files <- s3$list_objects_v2(Bucket = input_bucket, Prefix = input_folder)$Contents %>%
  purrr::map_chr("Key") %>%
  purrr::keep(~ grepl("\\.csv$", .))

cat("Found CSV files:", paste(csv_files, collapse = ", "), "\n")

if (length(csv_files) == 0) {
  cat("No CSV files found in the specified folder\n")
  stop("No CSV files to process")
}

# Function to clean a data frame
clean_data <- function(df, expected_column) {
  if (!expected_column %in% colnames(df)) {
    stop(paste("Expected column", expected_column, "not found in data frame. Available columns:", paste(colnames(df), collapse = ", ")))
  }

  cleaned_df <- df %>%
    filter(!is.na(.data[[expected_column]])) %>%
    mutate(new_column = .data[[expected_column]] * 2)

  return(cleaned_df)
}

# Process each CSV file
for (csv_file in csv_files) {
  cat("Processing file:", csv_file, "\n")

  tryCatch({
    # Read the CSV file from S3
    obj <- s3$get_object(Bucket = input_bucket, Key = csv_file)
    df <- read.csv(text = rawToChar(obj$Body), stringsAsFactors = FALSE)

    cat("Columns in CSV file:", paste(colnames(df), collapse = ", "), "\n")

    # Clean the data
    df_cleaned <- clean_data(df, "quantity")  # Replace "quantity" with the actual column name

    # Define the output file name
    output_file <- sub(input_folder, output_folder, csv_file)
    output_file <- sub("\\.csv$", "_cleaned.csv", output_file)

    cat("Saving cleaned file to:", output_file, "\n")

    # Write the cleaned data back to S3
    temp_file <- tempfile(fileext = ".csv")
    write.csv(df_cleaned, temp_file, row.names = FALSE)
    s3$put_object(Bucket = output_bucket, Key = output_file, Body = readBin(temp_file, "raw", file.info(temp_file)$size))
    file.remove(temp_file)

    cat("Successfully processed and saved cleaned file:", output_file, "\n")
  }, error = function(e) {
    cat("Error processing file:", csv_file, "Error:", conditionMessage(e), "\n")
  })
}

cat("Script completed successfully\n")
