import os
import yaml
import json
import boto3
from jinja2 import Template
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.ecs import ECSOperator
import pytz

def invoke_lambda(lambda_function_name, payload):
    client = boto3.client('lambda')
    response = client.invoke(
        FunctionName=lambda_function_name,
        InvocationType='RequestResponse',
        Payload=json.dumps(payload)
    )
    return json.loads(response['Payload'].read().decode('utf-8'))

def merge_configs(default_config, user_config):
    def recursive_merge(d, u):
        for k, v in u.items():
            if isinstance(v, dict):
                d[k] = recursive_merge(d.get(k, {}), v)
            else:
                d[k] = v
        return d
    return recursive_merge(default_config, user_config)

def parse_default_args(default_args):
    if 'start_date' in default_args:
        default_args['start_date'] = datetime.strptime(default_args['start_date'], '%Y-%m-%d')
    if 'retry_delay' in default_args:
        default_args['retry_delay'] = timedelta(minutes=int(default_args['retry_delay'].replace('m', '')))
    return default_args

def load_operator_template(operator_type):
    template_path = f'dags/operator_templates/{operator_type}.yaml'
    with open(template_path, 'r') as file:
        template = yaml.safe_load(file)
    return template['template']

def create_task(dag, operator):
    op_type = operator.get('type')
    op_id = operator.get('id')

    if not op_type or not op_id:
        raise AirflowException(f"Operator configuration is missing 'type' or 'id': {operator}")

    template_str = load_operator_template(op_type)
    template = Template(template_str)
    task_str = template.render(**operator, dag=dag)
    task = eval(task_str)
    return task

def create_dag(dag_id, config_path):
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)

    # Invoke the intelligence module (Lambda function) to determine resources
    lambda_function_name = "my_intelligence_lambda"
    payload = {"config": config}
    intelligence_output = invoke_lambda(lambda_function_name, payload)

    # Merge the intelligence output with the config
    intelligence_config = yaml.safe_load(intelligence_output)
    config = merge_configs(config, intelligence_config)

    default_config = config.get('dag', {})
    user_config = config.get('user_config', {}).get('dag', {})

    final_config = merge_configs(default_config, user_config)
    default_args = parse_default_args(final_config.get('default_args', {}))

    dag = DAG(
        dag_id=dag_id,
        description=final_config.get('description', 'A simple example DAG'),
        schedule_interval=final_config.get('schedule_interval', '@daily'),
        default_args=default_args,
        catchup=False
    )

    operators_config = config.get('operators', [])
    user_operators_config = config.get('user_config', {}).get('operators', [])
    operators_config.extend(user_operators_config)

    tasks = {op['id']: create_task(dag, op) for op in operators_config}

    for operator in operators_config:
        op_id = operator['id']
        dependencies = operator.get('dependencies', [])
        for dep in dependencies:
            if dep in tasks:
                tasks[dep] >> tasks[op_id]

    return dag

def write_dag_to_file(dag, dag_id, output_directory):
    dag_file_path = os.path.join(output_directory, f"{dag_id}.py")
    with open(dag_file_path, 'w') as f:
        f.write(f"from airflow import DAG\n")
        f.write(f"from airflow.operators.bash import BashOperator\n")
        f.write(f"from airflow.operators.python import PythonOperator, BranchPythonOperator\n")
        f.write(f"from airflow.providers.amazon.aws.operators.ecs import ECSOperator\n")
        f.write(f"from datetime import datetime, timedelta\n")
        f.write(f"import pytz\n\n")

        # Serialize the DAG
        f.write(f"default_args = {dag.default_args}\n")
        f.write(f"dag = DAG(dag_id='{dag.dag_id}', default_args=default_args, description='{dag.description}', schedule_interval='{dag.schedule_interval}', catchup={dag.catchup})\n\n")

        for task in dag.tasks:
            f.write(f"{task.task_id} = {task.__class__.__name__}(\n")
            f.write(f"    task_id='{task.task_id}',\n")
            f.write(f"    {task.__class__.__name__.lower()}_command='{task.bash_command}'" if isinstance(task, BashOperator) else f"    python_callable={task.python_callable},\n")
            f.write(f"    retries={task.retries},\n")
            f.write(f"    retry_delay=timedelta(minutes={task.retry_delay.seconds // 60}),\n")
            f.write(f"    dag=dag\n")
            f.write(f")\n\n")

        for task in dag.tasks:
            for downstream_task in task.downstream_task_ids:
                f.write(f"{task.task_id} >> {downstream_task}\n")

def generate_dags_from_s3(bucket_name, config_prefix, output_directory):
    s3_hook = S3Hook(aws_conn_id='aws_default')
    keys = s3_hook.list_keys(bucket_name=bucket_name, prefix=config_prefix)

    os.makedirs(output_directory, exist_ok=True)
    for key in keys:
        if key.endswith('.yaml'):
            config_path = s3_hook.download_file(key, bucket_name, '/tmp')
            dag_id = os.path.splitext(os.path.basename(key))[0]
            dag = create_dag(dag_id, config_path)
            write_dag_to_file(dag, dag_id, output_directory)

def main():
    bucket_name = 'your-bucket-name'  # Replace with your S3 bucket name
    config_prefix = 'dags/generated/'  # Prefix for generated DAGs
    output_directory = '/tmp/generated_dags'  # Local directory to write generated DAGs

    generate_dags_from_s3(bucket_name, config_prefix, output_directory)

if __name__ == "__main__":
    main()
