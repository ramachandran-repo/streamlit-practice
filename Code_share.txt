from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.utils.dates import days_ago
from airflow.models.dagbag import DagBag
import time

# Define your S3 bucket and key prefix
S3_BUCKET = 'your-s3-bucket'
S3_PREFIX = 'dags/'

def process_new_dag_file(**kwargs):
    ti = kwargs['ti']
    s3_object_key = ti.xcom_pull(task_ids='wait_for_new_dag')

    if not s3_object_key:
        raise ValueError("No S3 object key provided")
    
    dag_name = s3_object_key.split('/')[-1].replace('.py', '')

    # Wait for the DAG to be recognized by Airflow
    retries = 10
    delay = 5
    dag_bag = DagBag()

    for _ in range(retries):
        if dag_name in dag_bag.dags:
            break
        time.sleep(delay)
        dag_bag = DagBag()
    else:
        raise ValueError(f"DAG {dag_name} not found in Airflow after {retries * delay} seconds")

    # Trigger the new DAG
    from airflow.api.common.experimental.trigger_dag import trigger_dag
    trigger_dag(dag_id=dag_name)

default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
}

with DAG(
    dag_id='monitor_s3_and_trigger_dag',
    default_args=default_args,
    schedule_interval='*/10 * * * *',  # This sets the DAG to run every 10 minutes
    catchup=False,
) as dag:

    wait_for_new_dag = S3KeySensor(
        task_id='wait_for_new_dag',
        bucket_name=S3_BUCKET,
        bucket_key=f'{S3_PREFIX}*.py',
        wildcard_match=True,
        timeout=18 * 60 * 60,
        poke_interval=60,
    )

    process_new_dag_file = PythonOperator(
        task_id='process_new_dag_file',
        python_callable=process_new_dag_file,
        provide_context=True,
    )

    wait_for_new_dag >> process_new_dag_file
