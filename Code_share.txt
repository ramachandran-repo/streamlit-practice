dag:
  dag_id: example_dag
  description: Example DAG created from YAML config
  schedule_interval: "@daily"
  default_args:
    owner: "airflow"
    start_date: "2023-01-01"
    retries: 1
    retry_delay: "5m"

operators:
  - id: start_task
    type: BashOperator
    bash_command: "echo 'Starting the DAG'"

  - id: python_task
    type: PythonOperator
    python_callable: "lambda: print('Running Python Task')"
    op_args: []

  - id: end_task
    type: BashOperator
    bash_command: "echo 'Ending the DAG'"
    dependencies: 
      - python_task

user_config:
  dag:
    schedule_interval: "@hourly"
    default_args:
      retries: 2
      retry_delay: "10m"
  operators:
    - id: python_task
      type: PythonOperator
      python_callable: "lambda: print('Running Python Task with user config')"
      op_args: []



import os
import yaml
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException

def merge_configs(default_config, user_config):
    """
    Merge user configuration into the default configuration.
    """
    def recursive_merge(d, u):
        for k, v in u.items():
            if isinstance(v, dict):
                d[k] = recursive_merge(d.get(k, {}), v)
            else:
                d[k] = v
        return d

    return recursive_merge(default_config, user_config)

def parse_default_args(default_args):
    if 'start_date' in default_args:
        default_args['start_date'] = datetime.strptime(default_args['start_date'], '%Y-%m-%d')
    if 'retry_delay' in default_args:
        default_args['retry_delay'] = timedelta(minutes=int(default_args['retry_delay'].replace('m', '')))
    return default_args

def create_task(dag, operator):
    op_type = operator.get('type')
    op_id = operator.get('id')

    if not op_type or not op_id:
        raise AirflowException(f"Operator configuration is missing 'type' or 'id': {operator}")

    task_params = {
        'task_id': op_id,
        'retries': operator.get('retries', 1),
        'retry_delay': timedelta(minutes=int(operator.get('retry_delay', '5m').replace('m', ''))),
        'dag': dag
    }

    if op_type == 'BashOperator':
        return BashOperator(bash_command=operator['bash_command'], **task_params)
    elif op_type == 'PythonOperator':
        return PythonOperator(python_callable=eval(operator['python_callable']), op_args=operator.get('op_args', []), **task_params)
    else:
        raise AirflowException(f"Unsupported operator type: {op_type}")

def create_dag(dag_id, config_path):
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)

    default_config = config.get('dag', {})
    user_config = config.get('user_config', {}).get('dag', {})

    final_config = merge_configs(default_config, user_config)
    default_args = parse_default_args(final_config.get('default_args', {}))

    dag = DAG(
        dag_id=dag_id,
        description=final_config.get('description', 'A simple example DAG'),
        schedule_interval=final_config.get('schedule_interval', '@daily'),
        default_args=default_args,
        catchup=False
    )

    operators_config = config.get('operators', [])
    user_operators_config = config.get('user_config', {}).get('operators', [])
    operators_config.extend(user_operators_config)

    tasks = {op['id']: create_task(dag, op) for op in operators_config}

    for operator in operators_config:
        op_id = operator['id']
        dependencies = operator.get('dependencies', [])
        for dep in dependencies:
            if dep in tasks:
                tasks[dep] >> tasks[op_id]

    return dag

def generate_dags(config_directory):
    for config_file in os.listdir(config_directory):
        if config_file.endswith('.yaml'):
            config_path = os.path.join(config_directory, config_file)
            dag_id = os.path.splitext(config_file)[0]
            globals()[dag_id] = create_dag(dag_id, config_path)

if __name__ == "__main__":
    config_directory = '/usr/local/airflow/dags/'  # Adjust the path as needed
    generate_dags(config_directory)



name: Deploy DAG to Airflow

on:
  push:
    paths:
      - 'configurations/*.yaml'

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: pip install pyyaml boto3 apache-airflow

    - name: Generate DAG files
      run: |
        mkdir -p generated_dags
        python dag_factory.py configurations generated_dags

    - name: Upload DAGs to S3
      run: |
        python s3_uploader.py ${{ secrets.AWS_S3_BUCKET }} generated_dags dags
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}


